name: Kuber_cluster_monitoring

on:
  schedule:
    - cron: '*/60 * * * *'
  workflow_dispatch:

jobs:
  build:
    name: build
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v2
      - name: Check for failed pods
        uses: appleboy/ssh-action@v1.1.0
        with:
          proxy_host: ${{ secrets.JUMPHOST_IP }}
          proxy_port: ${{ secrets.JUMPHOST_PORT }}
          proxy_username: ${{ secrets.JUMPHOST_USER }}
          proxy_password: ${{ secrets.JUMPHOST_PASS }}
          host: ${{ secrets.K9S_SERVER_IP }}
          username: ${{ secrets.K9S_SERVER_USER }}
          password: ${{ secrets.K9S_SERVER_PASS }}
          script: |
            kubectl config use-context k8s --kubeconfig ~/.kube/config-k8s
            kubectl --kubeconfig ~/.kube/config-k8s get pods -A > k8s_status.log
            kubectl config use-context k3s --kubeconfig ~/.kube/config-k3s
            kubectl --kubeconfig ~/.kube/config-k3s get pods -A > k3s_status.log
            grep -E "Running" ~/k3s_status.log ~/k8s_status.log > Monitoring.txt
            if grep -q Running ~/Monitoring.txt; then
              echo "failed=true" >> $env:GITHUB_ENV
              cat ~/Monitoring.txt
            else
              echo "failed=false" >> $env:GITHUB_ENV
            fi

      - name: Slack Notification on Failure
	      if: env.failed == 'true' 
        uses: rtCamp/action-slack-notify@v2
		    env:
           SLACK_COLOR: ${{ failure }}
           SLACK_MESSAGE: "You have some failed pods in your cluster"
           SLACK_TITLE: Kuber Failed Pods
           SLACK_USERNAME: KuberCluster
           SLACK_WEBHOOK: ${{ secrets.HOOK }}
