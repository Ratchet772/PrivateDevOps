name: Kuber_cluster_monitoring

on:
  schedule:
    - cron: '*/60 * * * *'
  workflow_dispatch:

jobs:
  build:
    name: build
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v3
      - id: check_pods
        uses: appleboy/ssh-action@v1.1.0
        with:
          proxy_host: ${{ secrets.JUMPHOST_IP }}
          proxy_port: ${{ secrets.JUMPHOST_PORT }}
          proxy_username: ${{ secrets.JUMPHOST_USER }}
          proxy_password: ${{ secrets.JUMPHOST_PASS }}
          host: ${{ secrets.K9S_SERVER_IP }}
          username: ${{ secrets.K9S_SERVER_USER }}
          password: ${{ secrets.K9S_SERVER_PASS }}
          script: |
            kubectl config use-context k8s --kubeconfig ~/.kube/config-k8s
            kubectl --kubeconfig ~/.kube/config-k8s get pods -A > k8s_status.log
            kubectl config use-context k3s --kubeconfig ~/.kube/config-k3s
            kubectl --kubeconfig ~/.kube/config-k3s get pods -A > k3s_status.log
            grep -E "Error" ~/k3s_status.log ~/k8s_status.log > Monitoring.txt
            if grep -q Error ~/Monitoring.txt; then
              echo "failed=true" >> $GITHUB_OUTPUT
              cat ~/Monitoring.txt
            else
              echo "failed=false" >> $GITHUB_OUTPUT
            fi
      - name: Slack Notification on Failure
        if: ${{ steps.check_pods.outputs.failed == 'false' }}
        uses: rtCamp/action-slack-notify@v2   
        env:
          SLACK_COLOR: failure
          SLACK_MESSAGE: "You have some failed pods in your cluster"
          SLACK_TITLE: Kuber Failed Pods
          SLACK_USERNAME: KuberCluster
          SLACK_WEBHOOK: ${{ secrets.HOOK }}
